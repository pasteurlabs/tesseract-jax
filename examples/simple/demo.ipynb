{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesseract-JAX basic example: vector addition\n",
    "\n",
    "<div class=\"admonition note alert alert-warning\">\n",
    "<p class=\"admonition-title\">Note</p>\n",
    "\n",
    "All examples are expected to run from the `examples/<example_name>` directory of the [Tesseract-JAX repository](https://github.com/pasteurlabs/tesseract-jax).\n",
    "</div>\n",
    "\n",
    "Tesseract-JAX is a lightweight extension to Tesseract Core that makes Tesseracts look and feel like regular JAX primitives, and makes them jittable and differentiable.\n",
    "\n",
    "In this demo, you will learn how to:\n",
    "1. Build a Tesseract\n",
    "1. Access its endpoints via Tesseract-JAX's `apply_tesseract()` entrypoint\n",
    "1. Compose Tesseracts into more complex functions, blending multiple Tesseract applications with local operations\n",
    "1. Use JAX with the resulting function composition to perform JIT compilations, and / or autodifferentiate the function (via JVP, VJP, and explicit derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Tesseract\n",
    "\n",
    "You may build the Tesseract either via your command line, or running the cell below (you can skip running this if already built)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2K \u001b[1;2m[\u001b[0m\u001b[34mi\u001b[0m\u001b[1;2m]\u001b[0m Building image \u001b[33m...\u001b[0m\n",
      "\u001b[2K\u001b[37mâ ¼\u001b[0m \u001b[37mProcessing\u001b[0m\n",
      "\u001b[1A\u001b[2K \u001b[1;2m[\u001b[0m\u001b[34mi\u001b[0m\u001b[1;2m]\u001b[0m Built image sh\u001b[1;92ma256:c790\u001b[0m2d7912d7, \u001b[1m[\u001b[0m\u001b[32m'vectoradd_jax:latest'\u001b[0m\u001b[1m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"vectoradd_jax:latest\"]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Build vectoradd_jax Tesseract so we can use it below\n",
    "tesseract build vectoradd_jax/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Tesseract\n",
    "\n",
    "The main entrypoint to `tesseract_jax` is `apply_tesseract()`.\n",
    "\n",
    "Using the `vectoradd_jax` Tesseract image we built earlier, let's add two vectors together.\n",
    "The result should be:\n",
    "\n",
    "$$\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + 2 \\cdot \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 12 \\\\ 15 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing `tesseract_core.Tesseract`, and create an instance from the Tesseract image `vectoradd_jax` image we built.\n",
    "\n",
    "Tesseract-Core offers a `.serve()` method to keep the Tesseract alive, as an alternative to using `with` statements repeatedly to access the Tesseract endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesseract_core import Tesseract\n",
    "\n",
    "vectoradd = Tesseract.from_image(\"vectoradd_jax\")\n",
    "vectoradd.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations a specific Tesseract provides via endpoints may vary. We can introspect this using the `.available_endpoints` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apply',\n",
       " 'jacobian',\n",
       " 'jacobian_vector_product',\n",
       " 'vector_jacobian_product',\n",
       " 'health',\n",
       " 'input_schema',\n",
       " 'output_schema',\n",
       " 'abstract_eval']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectoradd.available_endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform our vector addition, we make use of the `apply_tesseract()` function mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API of `vectoradd_jax` has an `InputSchema` which takes two parameters `a` and `b`.\n",
    "\n",
    "`a` and `b` also share a schema, requiring a scalar `s` and vector `v` parameter (although `s = 1` by default).\n",
    "\n",
    "This can be passed to `apply_tesseract()` with a dict of dicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vector_add': {'normed_result': Array([0.42426407, 0.56568545, 0.70710677], dtype=float32),\n",
      "                'result': Array([ 9., 12., 15.], dtype=float32)},\n",
      " 'vector_min': {'normed_result': Array([-0.5025707 , -0.5743665 , -0.64616233], dtype=float32),\n",
      "                'result': Array([-7., -8., -9.], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tesseract_jax import apply_tesseract\n",
    "\n",
    "a = {\"v\": np.array([1.0, 2.0, 3.0], dtype=\"float32\")}\n",
    "b = {\n",
    "    \"v\": np.array([4.0, 5.0, 6.0], dtype=\"float32\"),\n",
    "    \"s\": np.array(2.0, dtype=\"float32\"),\n",
    "}\n",
    "\n",
    "outputs = apply_tesseract(vectoradd, inputs={\"a\": a, \"b\": b})\n",
    "\n",
    "pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `outputs['vector_add']` gives the expected $(9, 12, 15)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function composition via Tesseracts\n",
    "\n",
    "Here you'll learn how Tesseract-JAX enables you to compose chains of Tesseract evaluations, blended with local operations, while retaining the benefits of JAX.\n",
    "\n",
    "The function below applies `vectoradd` twice, *ie.* $(\\mathbf{a} + \\mathbf{b}) + \\mathbf{a}$, then performs local arithmetic on the outputs, applies `vectoradd` once more, and finally returns a single element of the result.\n",
    "\n",
    "As you will see, we can perform this fine-grained control of our Tesseract evaluation without sacrificing JAX's JIT compiler or autodifferentiation functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(16.135319, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fancy_operation(a: np.ndarray, b: np.ndarray) -> np.float32:\n",
    "    \"\"\"Fancy operation.\"\"\"\n",
    "    result = apply_tesseract(vectoradd, inputs={\"a\": a, \"b\": b})\n",
    "    result = apply_tesseract(\n",
    "        vectoradd, inputs={\"a\": {\"v\": result[\"vector_add\"][\"result\"]}, \"b\": b}\n",
    "    )\n",
    "    result = (\n",
    "        2.0 * result[\"vector_add\"][\"normed_result\"] + b[\"v\"]\n",
    "    )  # We can mix and match with local operations\n",
    "    result = apply_tesseract(vectoradd, inputs={\"a\": {\"v\": result}, \"b\": b})\n",
    "    return result[\"vector_add\"][\"result\"][1]\n",
    "\n",
    "\n",
    "fancy_operation(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is compatible with `jax.jit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(16.135319, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "jitted_op = jax.jit(fancy_operation)\n",
    "jitted_op(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use automatic differentiation\n",
    "\n",
    "This is possible with or without using JIT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing JVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primal=Array(16.135319, dtype=float32), jvp=Array(25.004124, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "primal, jvp = jax.jvp(fancy_operation, (a, b), (a, b))\n",
    "print(f\"{primal=}, {jvp=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `jvp` is the Jacobian of `fancy_operation` calculated in $(a,b)$ multiplied with the vector $(a, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'v': Array([-0.20733577,  0.56435245, -0.329298  ], dtype=float32)},\n",
      " {'s': Array(80.709854, dtype=float32),\n",
      "  'v': Array([-0.8293431, 50.663364 , -1.317192 ], dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "primal, vjp = jax.vjp(fancy_operation, a, b)\n",
    "pprint(vjp(primal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where each element of the tuple is associated to the corresponding argument `a` or `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the gradient\n",
    "\n",
    "Let's calculate the gradient of `fancy_operation` w.r.t. the `a` argument at the point $(a,b)$. `a` is the first argument, so we pass `jax.grad()` a parameter `argnums=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v': Array([-0.01284981,  0.03497622, -0.02040852], dtype=float32)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(fancy_operation, argnums=0)(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or similar to our VJP calculation, we could calculate the gradients for both parameters `a` and `b` simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'v': Array([-0.01284981,  0.03497622, -0.02040852], dtype=float32)},\n",
       " {'s': Array(5.002062, dtype=float32),\n",
       "  'v': Array([-0.05139923,  3.139905  , -0.08163408], dtype=float32)})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(fancy_operation, argnums=[0, 1])(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining JIT and autodiff!\n",
    "\n",
    "Notice that all of the above works also in conjunction with `jax.jit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v': Array([-0.01284981,  0.03497622, -0.02040852], dtype=float32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jit inside differentiation\n",
    "jax.jvp(jitted_op, (a, b), (a, b))\n",
    "\n",
    "primal, vjp = jax.vjp(jitted_op, a, b)\n",
    "vjp(primal)\n",
    "\n",
    "jax.grad(jitted_op)(a, b)\n",
    "\n",
    "# And jax.jit could also wrap everything\n",
    "jax.jit(jax.grad(jitted_op))(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teardown and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we kept the Tesseract alive using `.serve()`, now we need to stop it using `.teardown()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoradd.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "You've worked through building up differentiable pipelines with Tesseracts that blend seamlessly with JAX's API, thanks to Tesseract-JAX."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
